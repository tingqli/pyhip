import pyhip

'''

https://docs.nvidia.com/cutlass/media/docs/cpp/cute/01_layout.html
 Fundamentally, a Layout maps from coordinate space(s) to an index space.
 例如寄存器本质上是[threads,item]组成的2D-space，但是可以用来存放多维tensor数据,
 以存放2D tensor为例，可以是符合coalescing的按行划分，或者是符合MFMA摆放要求的
 这种不同就是layout体现出来，本质上回答一个问题就是：
    一个已知逻辑shape的数据，其中每个元素分别映射到[threads,item]中的哪个位置上去

 通过复杂的封装，这个映射可以做到用户无感知，而且这样还有一个好处是，可以方便的改动shape
 而不需要改动代码，很多tune的工作其实就是改动shape。

 因此用户代码需要以tile为单位描述其行为，才能做到这种无感知

 A Layout is a tuple of (Shape, Stride). Semantically, it implements a mapping
 from any coordinate within the Shape to an index via the Stride.

 A Layout can be composed with data – e.g., a pointer or an array – to create a Tensor.
 The index generated by the Layout is used to subscript an iterator to retrieve the 
 appropriate data.
 
 The fundamental use of a Layout is to map between coordinate space(s) defined
 by the Shape and an index space defined by the Stride. 

    Layout<Shape,Stride> const& layout
    layout(m,n) 返回坐标(m,n)对应的索引

    16,128 => 4h,4h,16v,8h
    16,128 => 4,64,8
 
    layout描述使用oneDNN的方式似乎更为直观，但是表达能力比较有限，比如交换两个4h无法表达
    最为灵活强大的表达似乎还是torch的 reshape+permute
    因此可以用这种语法表达layout

    这样的layout表达如何用于代码生成呢？
    例如这样的表达都需要生成一个加载语句，根据threadIdx.x决定源数据的位置
    但是只有寄存器Tile的layout不够，还需要源数据的layout，比如LDS可能是做了swizzle的

    外存数据也有layout，两者如何结合？
    结合之后还能正确推导出代码

    
    MFMA 4 warp：
        1x4摆放 :  Layout(16,128).reshape(16,4,4,8).permute(1,2,0,3).reshape(256, 8)
    
    nM = block_size_M//16
    nN = block_size_N//16
    nMw = nM//2
    nNw = nN//2

    考虑wave如何组合完成gemm，可能是2x2,1x4,4x1等
    这是由wave layout决定的，还可能是 1x2,2x1,1x1，然后各个wave在K维度上进行split最后再reduce
    这么多的配置不同，引起代码的生成都有不同，因此这些配置都是tune的参数
    这些参数给gemm是作为一个整体的，不是只给gemm算子一个，而是全部配合gemm的读入，每个wave的行为
    都要受到影响的。

    不考虑split_K，固定2x2 wave分解，也许可以给出一个可配置实现，可配置的只有每个wave内部负责的形状
    以及使用32x8还是16x16指令。

    一个gemm的micro kernel

    Layout(block_size_M, block_size_N)\
            .reshape(2,block_size_M//2, 2,block_size_N//2)\
            .permute(0,2,1,3)\
            .reshape(2, nMw, 2, nNw, 16, 16).reshape()

'''

class UGEMM:
    def __init__(self, J, 
                 mfma_MN:int,
                 wave_size:list,
                 wave_cnt:list,
                 K, N):
        self.K = K
        self.N = N
        
        wave_size_M, wave_size_N = wave_size
        wave_cnt_M, wave_cnt_N = wave_cnt
        assert mfma_MN in [16, 32]
        assert wave_size_M % mfma_MN == 0
        assert wave_size_N % mfma_MN == 0
        # *2 for 8-bf16/fp16 so DWORDx4 lane-size can be used
        self.mfma_K = (2*8 if mfma_MN == 32 else 2*16)

        # number of C/D regs per wave
        wave_nCM = wave_size_M // mfma_MN
        wave_nCN = wave_size_N // mfma_MN
        wave_nCK = 2

        # wnM,wnN: 2x2, 1x4, 4x1
        self.J = J
        self.mfma_MN = mfma_MN
        self.wave_size_M = wave_size_M
        self.wave_size_N = wave_size_N
        self.wave_cnt_M = wave_cnt_M
        self.wave_cnt_N = wave_cnt_N
        self.wave_cnt = wave_cnt_M * wave_cnt_N
        assert self.wave_cnt == 4
        
        self.wg_M = self.wave_size_M * self.wave_cnt_M
        self.wg_N = self.wave_size_N * self.wave_cnt_N
        self.wg_K = self.mfma_K * wave_nCK          # 2*(2*8) or 2*(2*16)

        assert self.K % self.wg_K == 0, f"{self.K=} {self.wg_K=}"
        # prefetch evenly distrubuted on all waves
        assert self.wg_M % self.wave_cnt == 0
        assert self.wg_N % self.wave_cnt == 0
        self.wave_nCM = wave_nCM
        self.wave_nCN = wave_nCN
        self.wave_nCK = wave_nCK

    def run(self, sgpr_a, sgpr_b, sgpr_c, M, debug_warp, skip_load):
        J = self.J

        sizeof_bf16 = 2
        sizeof_fp32 = 4
        sizeof_DWORDX4 = 16        
        buff_a = J.Buffer(sgpr_a, self.wg_M*self.K*sizeof_bf16)
        buff_b = J.Buffer(sgpr_b, self.wg_N*self.K*sizeof_bf16)

        #ldsA = J.LDSTensor([wg_M, wg_K], torch.bfloat16)
        #ldsB = J.LDSTensor([wg_N, wg_K], torch.bfloat16)

        LDSA_size = self.wg_M * self.wg_K * sizeof_bf16
        LDSB_size = self.wg_N * self.wg_K * sizeof_bf16
        ldsA = [J.alloc_lds(LDSA_size) for _ in range(2)]
        ldsB = [J.alloc_lds(LDSB_size) for _ in range(2)]

        # prefetch in memory-coalescing way
        assert (sizeof_bf16*self.wg_K) % sizeof_DWORDX4 == 0 # each lane prefetch DWORDx4 which is 8xhalf
        num_lanes_per_row = sizeof_bf16 * self.wg_K // sizeof_DWORDX4
        assert 64 % num_lanes_per_row == 0
        dw4_prefetch_MN = (256//num_lanes_per_row)
        assert dw4_prefetch_MN >= 1
        assert self.wg_M % dw4_prefetch_MN == 0
        assert self.wg_N % dw4_prefetch_MN == 0
        num_prefetch_M = self.wg_M // dw4_prefetch_MN
        num_prefetch_N = self.wg_N // dw4_prefetch_MN
        prefetch_Areg = J.gpr(num_prefetch_M, 4, "vu32")
        prefetch_Breg = J.gpr(num_prefetch_N, 4, "vu32")

        # 4-wave within a WG coorperatively loads VRAM data in DWORDx4
        # load_dwordx4(self, vdst, voffset, soffset, offset12=0):
        stride_AB = self.K * sizeof_bf16
        if skip_load:
            stride_AB = 0
        prefetch_voffset = J.gpr((J.threadIdx.x % num_lanes_per_row) * sizeof_DWORDX4 + \
                                 (J.threadIdx.x //num_lanes_per_row) * stride_AB)
        prefetch_soffsetA = J.gpr("su32")
        prefetch_soffsetB = J.gpr("su32")
        prefetch_step_size = (dw4_prefetch_MN)*stride_AB
        if skip_load:
            prefetch_step_size = 0
        def prefetchA(index):
            if index == 0:
                prefetch_soffsetA[0] = 0 if skip_load else id_prefetch * self.wg_K * sizeof_bf16 
            buff_a.load_dwordx4(prefetch_Areg[index], prefetch_voffset, prefetch_soffsetA)
            prefetch_soffsetA[0] = prefetch_soffsetA[0] + prefetch_step_size

        def prefetchB(index):
            if index == 0:
                prefetch_soffsetB[0] = 0 if skip_load else id_prefetch * self.wg_K * sizeof_bf16
            buff_b.load_dwordx4(prefetch_Breg[index], prefetch_voffset, prefetch_soffsetB)
            prefetch_soffsetB[0] = prefetch_soffsetB[0] + prefetch_step_size

        def swizzle(row, col):
            swizzle_row_div = 1 if self.mfma_MN == 16 else 2
            return (row//swizzle_row_div) ^ col

        # each swizzle generates a new vaddr pattern, precompute all of them
        ds_write_vaddr = J.gpr(max(num_prefetch_M, num_prefetch_N), "vu32")
        for i in range(max(num_prefetch_M, num_prefetch_N)):
            col = J.threadIdx.x % num_lanes_per_row
            row = (J.threadIdx.x // num_lanes_per_row) + i*dw4_prefetch_MN
            swizzle_col = swizzle(row, col) % (num_lanes_per_row)
            ds_write_vaddr[i] = J.gpr((row * (self.wg_K * sizeof_bf16)) + swizzle_col*(sizeof_DWORDX4))

        def ds_writeA(index, lds_base):
            J.ds_write_b128(ds_write_vaddr[index], prefetch_Areg[index], mod=f"offset:{lds_base}") #  vaddr, vdata offset gds
        def ds_writeB(index, lds_base):
            J.ds_write_b128(ds_write_vaddr[index], prefetch_Breg[index], mod=f"offset:{lds_base}") #  vaddr, vdata offset gds

        # each wave reads its own part
        ds_readA_vaddr = J.gpr(self.wave_nCM, self.wave_nCK, "vu32")
        ds_readB_vaddr = J.gpr(self.wave_nCN, self.wave_nCK, "vu32")
        # wave location
        warp_id = J.gpr("su32")
        J.v_readfirstlane_b32(warp_id, J.threadIdx.x[0] // 64)
        lane_id = J.gpr(J.threadIdx.x % 64)
        warp_id_m = warp_id // self.wave_cnt_N
        warp_id_n = warp_id % self.wave_cnt_N
        warp_offset_m = warp_id_m * self.wave_size_M
        warp_offset_n = warp_id_n * self.wave_size_N
        for m in range(self.wave_nCM):
            for k in range(self.wave_nCK):
                row = lane_id % self.mfma_MN + warp_offset_m + m*self.mfma_MN
                col = lane_id // self.mfma_MN + (k * self.mfma_K * sizeof_bf16) // sizeof_DWORDX4
                swizzle_col = swizzle(row, col) % (num_lanes_per_row)
                ds_readA_vaddr[m, k] = J.gpr((row * (self.wg_K * sizeof_bf16)) + swizzle_col*(sizeof_DWORDX4))

        for n in range(self.wave_nCN):
            for k in range(self.wave_nCK):
                row = lane_id % self.mfma_MN + warp_offset_n + n*self.mfma_MN
                col = lane_id // self.mfma_MN + (k * self.mfma_K * sizeof_bf16) // sizeof_DWORDX4
                swizzle_col = swizzle(row, col) % (num_lanes_per_row)
                ds_readB_vaddr[n, k] = J.gpr((row * (self.wg_K * sizeof_bf16)) + swizzle_col*(sizeof_DWORDX4))

        Creg_size = (self.mfma_MN * self.mfma_MN)//64
        mfma_C = self.J.gpr(self.wave_nCM, self.wave_nCN, Creg_size, f"af32")
        mfma_C[:] = 0 
        ABReg_size = (self.mfma_MN * self.mfma_K * 2//4)//64
        mfma_A = J.gpr(2, self.wave_nCM, self.wave_nCK, ABReg_size, "vbf16x2")
        mfma_B = J.gpr(2, self.wave_nCM, self.wave_nCK, ABReg_size, "vbf16x2")

        def ds_readA(i, m, k, lds_base):
            J.ds_read_b128(mfma_A[i,m,k], ds_readA_vaddr[m, k], mod=f"offset:{lds_base}") #  vaddr, vdata offset gds

        def ds_readB(i, n, k, lds_base):
            J.ds_read_b128(mfma_B[i,n,k], ds_readB_vaddr[n,k], mod=f"offset:{lds_base}") #  vaddr, vdata offset gds

        J.debug_setup((J.blockIdx.x[0] == 0) & (J.blockIdx.y[0] == 0) & (warp_id[0] == debug_warp))

        # prelog
        id_prefetch = 0
        for r in range(num_prefetch_M): prefetchA(r)
        for r in range(num_prefetch_N): prefetchB(r)
        J.s_waitcnt(mod="vmcnt(0)")

        #J.debug_log(prefetch_Areg[:], torch.bfloat16, "4v.8v.8h.8h")
        #J.debug_log(prefetch_Breg[:], torch.bfloat16, "4v.8v.8h.8h")

        id_ds_write = id_prefetch
        id_prefetch += 1
        for r in range(num_prefetch_M):
            ds_writeA(r, ldsA[id_ds_write & 1])
            prefetchA(r)
        for r in range(num_prefetch_N):
            ds_writeB(r, ldsB[id_ds_write & 1])
            prefetchB(r)

        J.s_waitcnt(mod="lgkmcnt(0)")
        J.s_barrier()
        J.s_waitcnt(mod=f"vmcnt(0)")

        id_ds_read = id_ds_write
        id_ds_write = id_prefetch
        id_prefetch += 1
        for r in range(num_prefetch_M):
            ds_writeA(r, ldsA[id_ds_write & 1])
            prefetchA(r)
        for r in range(num_prefetch_N):
            ds_writeB(r, ldsB[id_ds_write & 1])
            prefetchB(r)

        for k in range(0,self.wave_nCK//2):
            for m in range(self.wave_nCM):
                ds_readA(id_ds_read & 1, m, k, ldsA[id_ds_read & 1])
            for n in range(self.wave_nCN):
                ds_readB(id_ds_read & 1, n, k, ldsB[id_ds_read & 1])
        for k in range(self.wave_nCK//2, self.wave_nCK):
            for m in range(self.wave_nCM):
                ds_readA(id_ds_read & 1, m, k, ldsA[id_ds_read & 1])
            for n in range(self.wave_nCN):
                ds_readB(id_ds_read & 1, n, k, ldsB[id_ds_read & 1])

        num_ds_reads = self.wave_nCK * self.wave_nCM * self.wave_nCN
        num_ds_writes = num_prefetch_M + num_prefetch_N

        if 0:
            J.s_waitcnt(mod="lgkmcnt(0)")
            J.debug_log(mfma_A[0, 0], torch.bfloat16, "2h.4h.16v.8h")
            J.debug_log(mfma_A[0, 1], torch.bfloat16, "2h.4h.16v.8h")
            J.debug_log(mfma_A[0, 2], torch.bfloat16, "2h.4h.16v.8h")
            J.debug_log(mfma_A[0, 3], torch.bfloat16, "2h.4h.16v.8h")

            J.debug_log(mfma_B[0, 0], torch.bfloat16, "2h.4h.16v.8h")
            J.debug_log(mfma_B[0, 1], torch.bfloat16, "2h.4h.16v.8h")
            J.debug_log(mfma_B[0, 2], torch.bfloat16, "2h.4h.16v.8h")
            J.debug_log(mfma_B[0, 3], torch.bfloat16, "2h.4h.16v.8h")

        mfma_info={
            16:("v_mfma_f32_16x16x16_bf16",16),
            32:("v_mfma_f32_32x32x8_bf16",32)
        }
        mfma_name = mfma_info[self.mfma_MN][0]
        mfma_cycles = mfma_info[self.mfma_MN][1]

        id_compute = 0
        #for cur_k in range(0, self.K, self.wg_K):
        cur_k = J.gpr("su32")
        cur_k[0] = 0
        k_loop_cnt = self.K//self.wg_K
        assert k_loop_cnt % 2 == 0
        #with J.While(cur_k[0] < k_loop_cnt):
            # store 2 to LDSB & start prefetch 3
            # read 1 from LDSA & MFMA 0
            #cur_k[0] = cur_k[0] + 2
            #for unroll in range(2):
        for unroll in range(k_loop_cnt):
            id_ds_read = id_compute + 1
            id_ds_write = id_compute + 2
            id_prefetch = id_compute + 3 

            def mfma0_generator():
                for k in range(0,self.wave_nCK//2):
                    for m in range(self.wave_nCM):
                        for n in range(self.wave_nCN):
                            yield getattr(J, mfma_name)(mfma_C[m,n],
                                                mfma_B[id_compute&1, n, k, 0:1],
                                                mfma_A[id_compute&1, m, k, 0:1],
                                                mfma_C[m,n])
                    for m in range(self.wave_nCM):
                        for n in range(self.wave_nCN):
                            yield getattr(J, mfma_name)(mfma_C[m,n],
                                                mfma_B[id_compute&1, n, k, 2:3],
                                                mfma_A[id_compute&1, m, k, 2:3],
                                                mfma_C[m,n])

            def mfma1_generator():
                for k in range(self.wave_nCK//2, self.wave_nCK):
                    for m in range(self.wave_nCM):
                        for n in range(self.wave_nCN):
                            yield getattr(J, mfma_name)(mfma_C[m,n],
                                                mfma_B[id_compute&1, n, k, 0:1],
                                                mfma_A[id_compute&1, m, k, 0:1],
                                                mfma_C[m,n])
                    for m in range(self.wave_nCM):
                        for n in range(self.wave_nCN):
                            yield getattr(J, mfma_name)(mfma_C[m,n],
                                                mfma_B[id_compute&1, n, k, 2:3],
                                                mfma_A[id_compute&1, m, k, 2:3],
                                                mfma_C[m,n])

            mfma0 = mfma0_generator()
            mfma1 = mfma1_generator()

            def emit_mfma(gen, cycles):
                while cycles > 0:
                    if next(gen, None) is None:
                        break
                    cycles -= mfma_cycles

            J.s_waitcnt(mod=f"lgkmcnt({min(15,num_ds_reads//2)})")
            J.s_barrier()
            # ================= stage1 ds_write + buffer_load ==============
            J.s_waitcnt(mod=f"vmcnt({num_prefetch_N})")
            for r in range(num_prefetch_M):
                ds_writeA(r, ldsA[id_ds_write & 1])
                emit_mfma(mfma0, 16)
                prefetchA(r)
                emit_mfma(mfma0, 16*4)

            J.s_waitcnt(mod=f"vmcnt({num_prefetch_M})")
            for r in range(num_prefetch_N):
                ds_writeB(r, ldsB[id_ds_write & 1])
                emit_mfma(mfma0, 16)
                prefetchB(r)
                emit_mfma(mfma0, 16*4)

            emit_mfma(mfma0, 9999999)

            # ================= stage2 ds_read  ==============
            J.s_waitcnt(mod=f"lgkmcnt({min(15,num_ds_writes)})")
            for k in range(0,self.wave_nCK//2):
                for m in range(self.wave_nCM):
                    ds_readA(id_ds_read & 1, m, k, ldsA[id_ds_read & 1])
                    emit_mfma(mfma1, 16*2)
                for n in range(self.wave_nCN):
                    ds_readB(id_ds_read & 1, n, k, ldsB[id_ds_read & 1])
                    emit_mfma(mfma1, 16*2)

            for k in range(self.wave_nCK//2, self.wave_nCK):
                for m in range(self.wave_nCM):
                    ds_readA(id_ds_read & 1, m, k, ldsA[id_ds_read & 1])
                    emit_mfma(mfma1, 16*2)
                for n in range(self.wave_nCN):
                    ds_readB(id_ds_read & 1, n, k, ldsB[id_ds_read & 1])
                    emit_mfma(mfma1, 16*2)

            emit_mfma(mfma1, 9999999)

            id_compute += 1

        # write out
        J.s_waitcnt(mod=f"vmcnt(0)")
        if 0:
            J.debug_log(mfma_C[0], torch.float, "4h.4h.16v.4h")
            J.debug_log(mfma_C[1], torch.float, "4h.4h.16v.4h")
            J.debug_log(mfma_C[2], torch.float, "4h.4h.16v.4h")
            J.debug_log(mfma_C[3], torch.float, "4h.4h.16v.4h")

        buff_c = J.Buffer(sgpr_c, self.wg_M * N * sizeof_fp32)
        # store_dwordx4(self, vdata, voffset, soffset, offset12=0):
        for m in range(self.wave_nCM):
            for n in range(self.wave_nCN):
                row = lane_id % self.mfma_MN + warp_offset_m + m*self.mfma_MN
                col = lane_id // self.mfma_MN + n * (self.mfma_MN * sizeof_fp32 // sizeof_DWORDX4)
                voffset = J.gpr(row * (N*sizeof_fp32) + warp_offset_n*sizeof_fp32 + col*sizeof_DWORDX4)
                if self.mfma_MN == 16:
                    buff_c.store_dwordx4(mfma_C[m,n], voffset, 0)
                elif self.mfma_MN == 32:
                    for ni in range(4):
                        noff = ni*4
                        buff_c.store_dwordx4(mfma_C[m,n,noff:noff+3], voffset, 0, offset12=ni*8*sizeof_fp32)
                else:
                    assert 0

@pyhip.jit(with_debug_log=False)
def gemm_kernel(J, K, N,
                mfma_MN, wave_size, wave_cnt,
                debug_warp, skip_load,
                pA:"void*", pB:"void*", pC:"float*", M:"int"):
    #mfma_MN = 16
    #wave_size=[16*4,16*4]
    #wave_cnt=[2,2]
    blk_n = J.blockIdx.x
    blk_m = J.blockIdx.y
    gemm = UGEMM(J, mfma_MN, wave_size, wave_cnt, K, N)

    sizeof_bf16 = 2
    sizeof_fp32 = 4
    if not skip_load:
        pA[:] = pA[:] + blk_m * (gemm.wg_M * K * sizeof_bf16)
        pB[:] = pB[:] + blk_n * (gemm.wg_N * K * sizeof_bf16)
        pC[:] = pC[:] + (blk_m * (gemm.wg_M * N * sizeof_fp32) + blk_n * (gemm.wg_N * sizeof_fp32))

    gemm.run(pA, pB, pC, M, debug_warp, skip_load)
    return


import torch
torch.set_printoptions(linewidth=3000, sci_mode=False, edgeitems=8, )
torch.cuda.set_device(2)
torch.set_default_device('cuda')
torch.manual_seed(0)

CU_rows = 8 #best_rows
CU_cols = 10 #num_CU//CU_rows

debug_warp = 5
skip_load = 0
mfma_MN = 16
wave_size = [64, 64]
wave_cnt = [2,2]
wg_size = [wave_size[i]*wave_cnt[i] for i in range(2)]

M,N,K = wg_size[0]*8,wg_size[1]*10,64*50

A = torch.randn(M, K, dtype=torch.bfloat16)
B = torch.randn(N, K, dtype=torch.bfloat16)
out = torch.randn(M, N, dtype=torch.float)

gemm_kernel([N//wg_size[1], M//wg_size[0]],[256],
            K, N, mfma_MN, wave_size, wave_cnt,
            debug_warp, skip_load,
            A.data_ptr(), B.data_ptr(), out.data_ptr(), M)
ref_out = A @ B.t()

debug_warp_m = debug_warp//2
debug_warp_n = debug_warp%2
m0 = debug_warp_m*64 
n0 = debug_warp_n*64 
#print(B[m0:m0+64, :64])
#print(ref_out[m0:m0+64, n0:n0+64])

cur_out = out.to(torch.bfloat16)
if not torch.allclose(ref_out, cur_out, rtol=0.02, atol=0.02):
    print("====================ref_out")
    print(ref_out)
    print("====================cur_out")
    print(cur_out)
    idx = torch.where(torch.abs(ref_out - cur_out) > 0.02)
    if len(idx[0]):
        print(f'idx = {idx}\nref={ref_out[idx]}\ncur={cur_out[idx]}\n{len(idx[0])}')
    #assert 0
    color_id = 1
    color0 = f"\033[0;{30+(color_id % 8)}m"
    color1 = f"\033[0m"
    acc_info = color0 + "acc failed" + color1
else:
    acc_info = "acc passed"

#for i in range(4):
#    row = (i*4 + debug_warp)*8
#    print(A[row:row+8])

DATA_CLONES = 20
As = [torch.clone(A) for _ in range(DATA_CLONES)]
Bs = [torch.clone(B) for _ in range(DATA_CLONES)]
Cs = [torch.clone(out) for _ in range(DATA_CLONES)]

for i in range(10):
    di = i%DATA_CLONES
    with pyhip.cudaPerf(M*N*K*2, (M*K*2+K*N*2), name=f"gemm_{di}"):
        gemm_kernel([N//wg_size[1], M//wg_size[0]],[256],
                    K, N, mfma_MN, wave_size, wave_cnt,
                    debug_warp, skip_load,
                    As[di].data_ptr(), Bs[di].data_ptr(), Cs[di].data_ptr(), M)

torch.cuda.synchronize()
print(f"{M=} {N=} {K=} {mfma_MN=} {wave_size=} {wave_cnt=} {wg_size=} {acc_info}")
